{
  "metadata": {
    "name": "EmotiCam [Team: SingleGuy]<br>",
    "language_info": {
      "name": "JavaScipt",
      "version": "8.0"
    }
  },
  "jsnbversion": "v0.1",
  "cells": [
    {
      "code": "//>md\n# EmotiCam\n### What is EmotiCam?\nThis web application detects basic facial expression of people  (using AI ), like the ones given below:\n- ##### ğŸ˜² Surprised\n- ##### ğŸ˜ Happy\n- ##### ğŸ˜” Sad\n- ##### ğŸ¤¢ Disgusted\n- ##### ğŸ˜ Neutral\n- ##### ğŸ˜  Angry\n- ##### ğŸ˜¨ Fearful\nAlso, we have custom filters for you to try on the way. Enjoy!! \nNote: If the filters are not loading, save this file locally, try running the first block of code once again, and then refreshing the page. Load the file from the local cache and it should work.\nCredits: The code is build as part of Scribbler Pad Hackathon. Original submission is here: [xFaron/EmotiCamScribbler](https://github.com/xFaron/EmotiCamScribbler)",
      "status": "",
      "output": "<h1>EmotiCam</h1>\n<h3>What is EmotiCam?</h3>\n<p>This web application detects basic facial expression of people  (using AI ), like the ones given below:</p>\n<ul>\n<li><h5>ğŸ˜² Surprised</h5>\n</li>\n<li><h5>ğŸ˜ Happy</h5>\n</li>\n<li><h5>ğŸ˜” Sad</h5>\n</li>\n<li><h5>ğŸ¤¢ Disgusted</h5>\n</li>\n<li><h5>ğŸ˜ Neutral</h5>\n</li>\n<li><h5>ğŸ˜  Angry</h5>\n</li>\n<li><h5>ğŸ˜¨ Fearful</h5>\n</li>\n</ul>\n<p>Also, we have custom filters for you to try on the way. Enjoy!! \nNote: If the filters are not loading, save this file locally, try running the first block of code once again, and then refreshing the page. Load the file from the local cache and it should work.\nCredits: The code is build as part of Scribbler Pad Hackathon. Original submission is here: <a href=\"https://github.com/xFaron/EmotiCamScribbler\">xFaron/EmotiCamScribbler</a></p>\n",
      "type": "html"
    },
    {
      "code": "<h3>About the UI</h3>\n<p>The UI is made up of two sections. <br>\n The first section displays the camera feed. <br>\n The second section displays the current emotion detected by the AI.<br><br>\n \n   There are two buttons in the first section. One is to turn on and off the camera. Other button is to change the filters. You can click this button and it will cycle through all the availible filters</p>",
      "status": "",
      "output": "<h3>About the UI</h3>\n<p>The UI is made up of two sections. <br>\n The first section displays the camera feed. <br>\n The second section displays the current emotion detected by the AI.<br><br>\n \n   There are two buttons in the first section. One is to turn on and off the camera. Other button is to change the filters. You can click this button and it will cycle through all the availible filters</p>",
      "type": "html"
    },
    {
      "code": "<style>\n\t\t#container, #vidholder, #vidoverlay, #vidfeed, .controls, #camswitch , #emotionfeed {\n\t\t\tmargin: 0;\n\t\t\tpadding: 0;\n\t\t\tbox-sizing: border-box;\n\t\t}\n\n\t\t#container {\n\t\t\twidth: 100%;\n\t\t\theight: 100%;\n\t\t\tbackground-color: rgb(16, 16, 16);\n\t\t\tdisplay: flex;\n\t\t\tflex-direction: row;\n\t\t\talign-items: center;\n\t\t\tjustify-content: space-evenly;\n\t\t\toverflow: hidden;\n\t\t}\n\n\t\t#vidholder {\n\t\t\tposition: relative;\n\t\t\tdisplay: flex;\n\t\t\tjustify-content: center;\n\t\t\talign-items: center;\n\t\t\tbackground-color: rgb(32, 32, 32);\n\t\t\tborder-radius: 15px;\n\t\t\taspect-ratio: 16 / 9;\n\t\t\twidth: 75%;\n\t\t\toverflow: hidden;\n\t\t \tmargin: 15px 0;\n\t\t}\n\n\t\t#vidfeed {\n\t\t\theight: 100%;\n\t\t}\n\n\t\t#vidoverlay {\n\t\t\tposition: absolute;\n\t\t\tz-index: 5;\n\t\t}\n\n\t\t#emotionfeed {\n\t\t\twidth: 20%;\n\t\t\taspect-ratio: 1 / 1;\n\t\t\tborder-radius: 15px;\n\t\t\tbackground-color: rgb(0, 10, 0);\n\n\t\t\tcolor: white;\n\t\t\tdisplay: grid;\n\t\t\tplace-content: center;\n\t\t\tfont-size: 5em;\n\n\t\t}\n\n\t\t.controls {\n\t\t\tposition: absolute;\n\t\t\tbottom: 0;\n\t\t\tleft: 0;\n\t\t\tright: 0;\n\t\t\tdisplay: flex;\n\t\t\tflex-direction: row;\n\t\t\tjustify-content: center;\n\t\t\talign-items: center;\n\t\t\tz-index: 6;\n\t\t}\n\n\t\t.switch {\n\t\t\twidth: 50px;\n\t\t\theight: 50px;\n\t\t\tmargin: 5px;\n\t\t\tborder-radius: 50%;\n\t\t\tfont-size: 1.5em;\n\t\t\tbackground-color: rgb(64, 64, 64);\n\t\t\tdisplay: grid;\n\t\t\tplace-items: center;\n\t\t}\n\n</style>\n\n<div id=\"container\">\n\t<div id='vidholder'>\n\t\t<canvas id='vidoverlay'></canvas>\n\t\t<video id='vidfeed' autoplay></video>\n\t\t<div class=\"controls\">\n\t\t\t<div class='cam switch'>\n\t\t\t\t<svg width=\"30px\" height=\"30px\" viewBox=\"0 0 24 24\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\n\t\t\t\t<path d=\"M22 8.93137C22 8.32555 22 8.02265 21.8802 7.88238C21.7763 7.76068 21.6203 7.69609 21.4608 7.70865C21.2769 7.72312 21.0627 7.93731 20.6343 8.36569L17 12L20.6343 15.6343C21.0627 16.0627 21.2769 16.2769 21.4608 16.2914C21.6203 16.3039 21.7763 16.2393 21.8802 16.1176C22 15.9774 22 15.6744 22 15.0686V8.93137Z\" stroke=\"#000000\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\n\t\t\t\t<path d=\"M2 9.8C2 8.11984 2 7.27976 2.32698 6.63803C2.6146 6.07354 3.07354 5.6146 3.63803 5.32698C4.27976 5 5.11984 5 6.8 5H12.2C13.8802 5 14.7202 5 15.362 5.32698C15.9265 5.6146 16.3854 6.07354 16.673 6.63803C17 7.27976 17 8.11984 17 9.8V14.2C17 15.8802 17 16.7202 16.673 17.362C16.3854 17.9265 15.9265 18.3854 15.362 18.673C14.7202 19 13.8802 19 12.2 19H6.8C5.11984 19 4.27976 19 3.63803 18.673C3.07354 18.3854 2.6146 17.9265 2.32698 17.362C2 16.7202 2 15.8802 2 14.2V9.8Z\" stroke=\"#000000\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/>\n\t\t\t\t</svg>\n\t\t\t</div>\n\t\t\t<div class=\"switch cosmetic\">ğŸš«</div>\n\t\t</div>\n\t</div>\n\t<div id=\"emotionfeed\">ğŸ˜</div>\n  </div>",
      "status": "",
      "output": "<style>\n\t\t#container, #vidholder, #vidoverlay, #vidfeed, .controls, #camswitch , #emotionfeed {\n\t\t\tmargin: 0;\n\t\t\tpadding: 0;\n\t\t\tbox-sizing: border-box;\n\t\t}\n\n\t\t#container {\n\t\t\twidth: 100%;\n\t\t\theight: 100%;\n\t\t\tbackground-color: rgb(16, 16, 16);\n\t\t\tdisplay: flex;\n\t\t\tflex-direction: row;\n\t\t\talign-items: center;\n\t\t\tjustify-content: space-evenly;\n\t\t\toverflow: hidden;\n\t\t}\n\n\t\t#vidholder {\n\t\t\tposition: relative;\n\t\t\tdisplay: flex;\n\t\t\tjustify-content: center;\n\t\t\talign-items: center;\n\t\t\tbackground-color: rgb(32, 32, 32);\n\t\t\tborder-radius: 15px;\n\t\t\taspect-ratio: 16 / 9;\n\t\t\twidth: 75%;\n\t\t\toverflow: hidden;\n\t\t \tmargin: 15px 0;\n\t\t}\n\n\t\t#vidfeed {\n\t\t\theight: 100%;\n\t\t}\n\n\t\t#vidoverlay {\n\t\t\tposition: absolute;\n\t\t\tz-index: 5;\n\t\t}\n\n\t\t#emotionfeed {\n\t\t\twidth: 20%;\n\t\t\taspect-ratio: 1 / 1;\n\t\t\tborder-radius: 15px;\n\t\t\tbackground-color: rgb(0, 10, 0);\n\n\t\t\tcolor: white;\n\t\t\tdisplay: grid;\n\t\t\tplace-content: center;\n\t\t\tfont-size: 5em;\n\n\t\t}\n\n\t\t.controls {\n\t\t\tposition: absolute;\n\t\t\tbottom: 0;\n\t\t\tleft: 0;\n\t\t\tright: 0;\n\t\t\tdisplay: flex;\n\t\t\tflex-direction: row;\n\t\t\tjustify-content: center;\n\t\t\talign-items: center;\n\t\t\tz-index: 6;\n\t\t}\n\n\t\t.switch {\n\t\t\twidth: 50px;\n\t\t\theight: 50px;\n\t\t\tmargin: 5px;\n\t\t\tborder-radius: 50%;\n\t\t\tfont-size: 1.5em;\n\t\t\tbackground-color: rgb(64, 64, 64);\n\t\t\tdisplay: grid;\n\t\t\tplace-items: center;\n\t\t}\n\n</style>\n\n<div id=\"container\">\n\t<div id=\"vidholder\">\n\t\t<canvas id=\"vidoverlay\" width=\"601\" height=\"451\" style=\"width: 601px; height: 451px;\"></canvas>\n\t\t<video id=\"vidfeed\" autoplay=\"\"></video>\n\t\t<div class=\"controls\">\n\t\t\t<div class=\"cam switch\" style=\"background-color: rgb(64, 64, 64);\">\n\t\t\t\t<svg width=\"30px\" height=\"30px\" viewBox=\"0 0 24 24\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\n\t\t\t\t<path d=\"M22 8.93137C22 8.32555 22 8.02265 21.8802 7.88238C21.7763 7.76068 21.6203 7.69609 21.4608 7.70865C21.2769 7.72312 21.0627 7.93731 20.6343 8.36569L17 12L20.6343 15.6343C21.0627 16.0627 21.2769 16.2769 21.4608 16.2914C21.6203 16.3039 21.7763 16.2393 21.8802 16.1176C22 15.9774 22 15.6744 22 15.0686V8.93137Z\" stroke=\"#000000\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\"></path>\n\t\t\t\t<path d=\"M2 9.8C2 8.11984 2 7.27976 2.32698 6.63803C2.6146 6.07354 3.07354 5.6146 3.63803 5.32698C4.27976 5 5.11984 5 6.8 5H12.2C13.8802 5 14.7202 5 15.362 5.32698C15.9265 5.6146 16.3854 6.07354 16.673 6.63803C17 7.27976 17 8.11984 17 9.8V14.2C17 15.8802 17 16.7202 16.673 17.362C16.3854 17.9265 15.9265 18.3854 15.362 18.673C14.7202 19 13.8802 19 12.2 19H6.8C5.11984 19 4.27976 19 3.63803 18.673C3.07354 18.3854 2.6146 17.9265 2.32698 17.362C2 16.7202 2 15.8802 2 14.2V9.8Z\" stroke=\"#000000\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\"></path>\n\t\t\t\t</svg>\n\t\t\t</div>\n\t\t\t<div class=\"switch cosmetic\">ğŸ‘€</div>\n\t\t</div>\n\t</div>\n\t<div id=\"emotionfeed\">ğŸ˜</div>\n  </div>",
      "type": "html"
    },
    {
      "code": "//Filter images\n//Sunglass\nvar sunglassImg = new Image();\nsunglassImg.src = \"https://raw.githubusercontent.com/xFaron/Emotion-AR-Hackathon-/main/assets/ThugGlasses.png\";\nvar sunglass_data = {\n\twidth: sunglassImg.width,\n\theight: sunglassImg.height,\n\tleftEye : {x : 9, y : 3}, //left glass eye coord\n\trightEye : {x : 17, y : 3} //right glass eye coord\n};\n//Heart Eyes\nvar heartEyesImg = new Image();\nheartEyesImg.src = \"https://raw.githubusercontent.com/xFaron/Emotion-AR-Hackathon-/main/assets/HeartEyes.png\";\nvar heartEyes_data = {\n\twidth: heartEyesImg.width,\n\theight: heartEyesImg.height,\n\tleftEye : {x : 5, y : 3}, //left glass eye coord\n\trightEye : {x : 12, y : 3} //right glass eye coord\n};\n//Eyes\nvar EyesImg = new Image();\nEyesImg.src = \"https://raw.githubusercontent.com/xFaron/Emotion-AR-Hackathon-/main/assets/Eyes.png\";\nvar Eyes_data = {\n\twidth: EyesImg.width,\n\theight: EyesImg.height,\n\tleftEye : {x : 8, y : 6}, //left glass eye coord\n\trightEye : {x : 16, y : 6} //right glass eye coord\n};\n//NerdGlasses\nvar nerdGlassesImg = new Image();\nnerdGlassesImg.src = \"https://raw.githubusercontent.com/xFaron/Emotion-AR-Hackathon-/main/assets/NerdGlasses.png\";\nvar nerdGlasses_data = {\n\twidth: nerdGlassesImg.width,\n\theight: nerdGlassesImg.height,\n\tleftEye : {x : 7, y : 6}, //left glass eye coord\n\trightEye : {x : 17, y : 6} //right glass eye coord\n};\n//NerdGlasses2\nvar nerdGlasses2Img = new Image();\nnerdGlasses2Img.src = \"https://raw.githubusercontent.com/xFaron/Emotion-AR-Hackathon-/main/assets/NerdGlasses2.png\";\nvar nerdGlasses2_data = {\n\twidth: nerdGlasses2Img.width,\n\theight: nerdGlasses2Img.height,\n\tleftEye : {x : 8, y : 6}, //left glass eye coord\n\trightEye : {x : 16, y : 6} //right glass eye coord\n};\n//NerdStache\nvar nerdStacheImg = new Image();\nnerdStacheImg.src = \"https://raw.githubusercontent.com/xFaron/Emotion-AR-Hackathon-/main/assets/NerdStache.png\";\nvar nerdStache_data = {\n\twidth: nerdStacheImg.width,\n\theight: nerdStacheImg.height,\n\tmouthWidth: 7,\n\tcenter: {x : 14, y : 3}\n};",
      "status": "[5]<br><span style=\"font-size:8px\">2ms<span></span></span>",
      "output": "https://raw.githubusercontent.com/xFaron/Emotion-AR-Hackathon-/main/assets/NerdStache.png <br>",
      "type": "code"
    },
    {
      "code": "<p> Defining variables </p>",
      "status": "",
      "output": "<p> Defining variables </p>",
      "type": "html"
    },
    {
      "code": "scrib.loadScript(\"https://cdn.jsdelivr.net/npm/@vladmandic/face-api@1/dist/face-api.js\");\n\nvar vidfeed = document.getElementById(\"vidfeed\");\nvar canvas = document.getElementById(\"vidoverlay\");\nvar camBtn = document.getElementsByClassName(\"cam\")[0];\nvar cosmeticBtn = document.getElementsByClassName(\"cosmetic\")[0];\nvar emotionFeed = document.getElementById(\"emotionfeed\");\nvar ctx = canvas.getContext('2d');\n\n// Other constants\nvar camBtnOffColor = 'red';\nvar camBtnOnColor = 'rgb(64, 64, 64)';\n\nvar cosmetics = [\n\t{type: [], img: [], data: [], emoji: \"ğŸš«\"},\n\t{type: ['eye'], img: [sunglassImg], data: [sunglass_data], emoji: \"ğŸ˜\"},\n\t{type: ['eye'], img: [heartEyesImg], data: [heartEyes_data], emoji: \"ğŸ˜\"},\n\t{type: ['eye'], img: [EyesImg], data: [Eyes_data], emoji: \"ğŸ‘€\"},\n\t{type: ['eye', 'stache'], img: [nerdGlassesImg, nerdStacheImg], data: [nerdGlasses_data, nerdStache_data], emoji: \"ğŸ¥¸\"},\n\t{type: ['eye', 'stache'], img: [nerdGlasses2Img, nerdStacheImg], data: [nerdGlasses2_data, nerdStache_data], emoji: \"ğŸ‘“\"}\n];\nvar currentCosmeticNo = 0;\n\nvar emotion = {\n\tsurprised: \"ğŸ˜²\",\n\thappy: \"ğŸ˜\",\n\tsad: \"ğŸ˜”\",\n\tdisgusted: \"ğŸ¤¢\",\n\tneutral: \"ğŸ˜\",\n\tangry: \"ğŸ˜ \",\n\tfearful : \"ğŸ˜¨\"\n};\nvar last_emotion;\n\n//Video stream variables\nvar vidSize = {\n\twidth: vidfeed.clientWidth,\n\theight: vidfeed.clientHeight\n};\nvar camOn = false;\nvar streamVal;",
      "status": "[6]<br><span style=\"font-size:8px\">495ms<span></span></span>",
      "output": "",
      "type": "code"
    },
    {
      "code": "<p> Defining fuctions </p>",
      "status": "",
      "output": "<p> Defining fuctions </p>",
      "type": "html"
    },
    {
      "code": "//To start the camera\nfunction startCam() {\n\tnavigator.mediaDevices.getUserMedia({ video: true })\n\t\t.then((stream) => {\n\t\t\tstreamVal = stream\n\t\t\tcamOn = true;\n\t\t\tvidfeed.srcObject = stream; // This part sends the MediaStream to video\n\t\t\tvidfeed.addEventListener(\"playing\", () => {\n\t\t\t\tresizeCanvas()\n\t\t\t})\n\t\t\tcamBtn.style[\"background-color\"] = camBtnOnColor;\n\t\t})\n\t\t.catch((err) => {\n\t\t\tconsole.log(\"Error: \", err); // To Handle vid error [TODO]\n\t\t});\n}\n\n// To turn off web cam\nfunction stopCam() {\n\tif (streamVal){\n\t\tlet tracks = streamVal.getTracks();\n\t\ttracks.forEach((track) => {track.stop()})\n\t\tcamOn = false;\n\t\tcamBtn.style[\"background-color\"] = camBtnOffColor;\n\t}\n}\n\n//Loads models\nasync function loadModels() {\n\ttry {\n\t\tscrib.show(\"Loading models\");\n\t\tawait faceapi.loadTinyFaceDetectorModel('https://raw.githubusercontent.com/justadudewhohacks/face-api.js/master/weights/');\n\t\tawait faceapi.loadFaceLandmarkModel('https://raw.githubusercontent.com/justadudewhohacks/face-api.js/master/weights/');\n\t\tawait faceapi.loadFaceExpressionModel('https://raw.githubusercontent.com/justadudewhohacks/face-api.js/master/weights/');\n\t\tscrib.show(\"Models loaded succefully!\");\n\t} catch (err) {\n\t\tscrib.show(\"Error\") ;\n\t}\n}\n\n// Detects the faces by inputting the vid stream, and draws boxes\nasync function detectFaces() {\n\tvar detections = await faceapi.detectAllFaces(vidfeed, new faceapi.TinyFaceDetectorOptions()).withFaceLandmarks().withFaceExpressions(); // Detect facce with TinyModel\n\tvar detectionsResized  = faceapi.resizeResults(detections, vidSize); // Resizes results for provided canvas (precaution)\n\t\n\tdetectionsResized.forEach((face) => {\n\t\t//Reading emotion\n\t\tlet top_emotion, top_emotion_val = 0; \n\t\tctx.clearRect(0, 0, vidSize.width, vidSize.height);\n\t\tObject.keys(face.expressions).forEach(emotion => {\n\t\t\tif (top_emotion_val < face.expressions[emotion]){\n\t\t\t\ttop_emotion = emotion;\n\t\t\t\ttop_emotion_val = face.expressions[emotion];\n\t\t\t}\n\t\t})\n\n\t\t//Changing feed based on emotion\n\t\tif (last_emotion != top_emotion){\n\t\t\tlast_emotion = top_emotion;\n\t\t\temotionFeed.innerHTML = emotion[top_emotion];\n\t\t}\n\n\t\tctx.beginPath();\n\n\t\t//Selecting cosmetics\n\t\tlet wearable = cosmetics[currentCosmeticNo];\n\n\t\tlet loaded = true;\n\t\twearable.img.forEach((img) => {\n\t\t\tloaded = loaded && img.complete\n\t\t})\n\n\t\tif (loaded){\n\t\t\twearable.type.forEach((type, index) => {\n\t\t\t\tif (type == 'eye'){\n\n\t\t\t\t\t// Finding left eye coords\n\t\t\t\t\tlet data = face.landmarks.getLeftEye();\n\t\t\t\t\tlet leftEye = {x:0, y:0};\n\t\t\t\t\tdata.forEach((point) => {\n\t\t\t\t\t\tleftEye.x += point.x;\n\t\t\t\t\t\tleftEye.y += point.y;\n\t\t\t\t\t})\n\t\t\t\t\tleftEye.x /= 6;\n\t\t\t\t\tleftEye.y /= 6;\n\n\t\t\t\t\t// Finding right eye coords\n\t\t\t\t\tdata = face.landmarks.getRightEye();\n\t\t\t\t\tlet rightEye = {x:0, y:0};\n\t\t\t\t\tdata.forEach((point) => {\n\t\t\t\t\t\trightEye.x += point.x;\n\t\t\t\t\t\trightEye.y += point.y;\n\t\t\t\t\t})\n\t\t\t\t\trightEye.x /= 6;\n\t\t\t\t\trightEye.y /= 6;\n\n\t\t\t\t\tlet calc = calcEyeImgLocn(wearable.data[index], leftEye, rightEye);\n\t\t\t\t\tctx.imageSmoothingEnabled = false;\n\t\t\t\t\tctx.drawImage(wearable.img[index], calc.x, calc.y, calc.width, calc.height);\n\n\t\t\t\t} else if (type == 'stache'){\n\n\t\t\t\t\t// Finding top and central point on mouth\n\t\t\t\t\tdata = face.landmarks.getMouth();\n\t\t\t\t\tlet mouthTopPoint = {x:0, y:0}, mouthWidth;\n\t\t\t\t\tlet sumX = 0, maxY = null, maxX = null, minX = null;\n\t\t\t\t\tdata.forEach((point) => {\n\t\t\t\t\t\tif (maxY == null || maxY > point.y){\n\t\t\t\t\t\t\tmaxY = point.y;\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\tif (maxX == null || maxX < point.x){\n\t\t\t\t\t\t\tmaxX = point.x;\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\tif (maxY == null || maxX > point.x){\n\t\t\t\t\t\t\tminX = point.x;\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\tsumX += point.x;\n\t\t\t\t\t})\n\t\t\t\t\tmouthTopPoint.x = sumX / data.length; // Taking avg of all x values\n\t\t\t\t\tmouthTopPoint.y = maxY;\n\t\t\t\t\tmouthWidth = Math.abs(maxX - minX);\n\n\t\t\t\t\t//Finding bottom and central point on nose\n\t\t\t\t\tdata = face.landmarks.getNose();\n\t\t\t\t\tlet noseBottomPoint = {x:0, y:0};\n\t\t\t\t\tsumX = 0, minY = null;\n\t\t\t\t\tdata.forEach((point) => {\n\t\t\t\t\t\tif (minY == null || minY < point.y){\n\t\t\t\t\t\t\tminY = point.y\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\tsumX += point.x;\n\t\t\t\t\t})\n\t\t\t\t\tnoseBottomPoint.x = sumX / data.length; // Taking avg of all x values\n\t\t\t\t\tnoseBottomPoint.y = minY;\n\n\t\t\t\t\tlet calc = calcStacheImgLocn(wearable.data[index], noseBottomPoint, mouthTopPoint, mouthWidth);\n\t\t\t\t\tctx.imageSmoothingEnabled = false;\n\t\t\t\t\tctx.drawImage(wearable.img[index], calc.x, calc.y, calc.width, calc.height);\n\t\t\t\t}\n\t\t\t})\n\t\t}\n\n\t\tctx.closePath();\n\t});\n}\n\n//Sunglass type image locn finder\nfunction calcEyeImgLocn(img_data, leftEye, rightEye) {\n\n\tvar req_vals = { x: 0, y: 0, width: 0, height: 0};\n\n\tvar dist_btw_eyes_img = Math.abs(img_data.leftEye.x - img_data.rightEye.x);\n\tvar dist_btw_eyes_actual = Math.abs(leftEye.x - rightEye.x);\n\n\tvar k = (dist_btw_eyes_actual/dist_btw_eyes_img);\n\n\treq_vals.x = leftEye.x - (img_data.leftEye.x * k);\n\treq_vals.y = leftEye.y - (img_data.leftEye.y * k);\n\treq_vals.width = img_data.width * k;\n\treq_vals.height = img_data.height * k;\n\n\treturn req_vals;\n}\n\n//Stache type image locn finder\nfunction calcStacheImgLocn(img_data, noseBottomPoint, mouthTopPoint, mouthWidth){\n\tlet req_vals = { x: 0, y: 0, width: 0, height: 0}; \n\tlet k = mouthWidth / img_data.mouthWidth;\n\n\t// Taking the avg of the noseBottom and mouthTop point to get the central posn of our stache, but as we need the top left corner point, shifting from there by k * img_data.center(vec), k scales it up to appropriate dimensions\n\treq_vals.x = (noseBottomPoint.x + mouthTopPoint.x)/2 - (img_data.center.x * k);\n\treq_vals.y = (noseBottomPoint.y + mouthTopPoint.y)/2 - (img_data.center.y * k);\n\n\treq_vals.width = img_data.width * k;\n\treq_vals.height = img_data.height * k;\n\n\treturn req_vals;\n}\n\nfunction resizeCanvas() {\n\tvidSize = {\n\t\twidth: vidfeed.clientWidth,\n\t\theight: vidfeed.clientHeight\n\t};\n\tcanvas.style.width = `${vidSize.width}px`;\n\tcanvas.style.height = `${vidSize.height}px`;\n\tcanvas.width = vidSize.width;\n\tcanvas.height = vidSize.height;\n}\n  \nfunction ifLoaded() {\n\treturn new Promise(resolve => {\n\t\tconst interval = setInterval(() => {\n\t\t\tlet loaded = true;\n\t\t\tcosmetics.forEach(wearable => {\n\t\t\t\twearable.img.forEach(img => {\n\t\t\t\t\tloaded = loaded && img.complete;\n\t\t\t\t})\n\t\t\t})\n\n\t\t\tif (loaded) {\n\t\t\t\tclearInterval(interval);\n\t\t\t\tresolve();\n\t\t\t}\n\t\t}, 100);\n\t})\n}\n\nfunction loop() {\n\tif (camOn) {\n\t\tdetectFaces();\n\t}\n\trequestAnimationFrame(loop);\n}\n  \nasync function init() {\n\tconsole.log(\"Loading images\");\n\tawait loadModels()\n\tawait ifLoaded();\n\tconsole.log(\"Images loaded!\");\n\tloop();\n}\n\nwindow.addEventListener('resize', () => {\n\t// When the window resizes, the canvas also has to be resized\n\tresizeCanvas();\n})\n\ncamBtn.addEventListener('click', () => {\n\tif (camOn){\n\t\tstopCam();\n\t} else {\n\t\tstartCam();\n\t}\n})\n  \ncosmeticBtn.addEventListener('click', () => {\n\tif (currentCosmeticNo + 1 == cosmetics.length){\n\t\tcurrentCosmeticNo = 0;\n\t} else {\n\t\tcurrentCosmeticNo++;\n\t}\n\n\tcosmeticBtn.innerHTML = `${cosmetics[currentCosmeticNo].emoji}`;\n})",
      "status": "[7]<br><span style=\"font-size:8px\">0ms<span></span></span>",
      "output": "",
      "type": "code"
    },
    {
      "code": "startCam();\ninit();",
      "status": "[8]<br><span style=\"font-size:8px\">3.736s<span></span></span>",
      "output": "Loading models <br>Models loaded succefully! <br>",
      "type": "code"
    }
  ],
  "source": "https://github.com/gopi-suvanam/jsnb",
  "run_on_load": false
}