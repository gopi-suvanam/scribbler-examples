{
  "metadata": {
    "name": "Web LLM Example",
    "language_info": {
      "name": "JavaScipt",
      "version": "8.0"
    }
  },
  "jsnbversion": "v0.1",
  "cells": [
    {
      "code": "//>md\n# LLM in the borwser using WebLLM\n\nThis is an example of how an LLM can be used safely in the browser without the need for servers or external services. This proves one can use AI in edge computing. This particular notebook requires GPU with atleast 8GB of VRAM.\n\nReferance: [Web LLM Github](https://github.com/mlc-ai/web-llm)\n\t\t\t\t\t\t\t\nNote: The notebook takes 3-4 four minutes to load the LLM for the first time. Next time onwards it will only take 25 seconds as the model is cached. Simple queries are taking 3-5 seconds.",
      "status": "",
      "output": "<h1>LLM in the borwser using WebLLM</h1>\n<p>This is an example of how an LLM can be used safely in the browser without the need for servers or external services. This proves one can use AI in edge computing. This particular notebook requires GPU with atleast 8GB of VRAM.</p>\n<p>Referance: <a href=\"https://github.com/mlc-ai/web-llm\">Web LLM Github</a></p>\n<p>Note: The notebook takes 3-4 four minutes to load the LLM for the first time. Next time onwards it will only take 25 seconds as the model is cached. Simple queries are taking 3-5 seconds.</p>\n",
      "type": "html"
    },
    {
      "code": "\nif(scrib.isSandboxed()) scrib.show(\"<p style='color:red' >You will have to take the notebook out of sandbox by clicking the red â¤¯ button at the top right corner and then enter the phrase 'I trust'. Do this only if you got the notebook from a trusted source</p>\")",
      "status": "[1]<br><span style=\"font-size:8px\">0ms<span></span></span>",
      "output": "",
      "type": "code"
    },
    {
      "code": "webllm = await import(\"https://cdn.jsdelivr.net/npm/@mlc-ai/web-llm@0.2.72/lib/index.min.js\");",
      "status": "[2]<br><span style=\"font-size:8px\">55ms<span></span></span>",
      "output": "",
      "type": "code"
    },
    {
      "code": "<h2>Loading Progress</h2>\n<progress id=\"loading-progress-bar\" value=\"0\" max=\"1\"></progress>\n<span id=\"loading-progress-text\">0%</span>\n",
      "status": "",
      "output": "<h2>Loading Progress</h2>\n<progress id=\"loading-progress-bar\" value=\"0.9736842105263158\" max=\"1\"></progress>\n<span id=\"loading-progress-text\">97%</span>\n",
      "type": "html"
    },
    {
      "code": "\n\n// Callback function to update model loading progress\nconst progressBar = document.getElementById('loading-progress-bar');\nconst progressText = document.getElementById('loading-progress-text');\nconst initProgressCallback = (initProgress) => {\n  const progressValue = initProgress.progress;\n  progressBar.value = progressValue;\n  progressText.textContent = `${Math.round(progressValue * 100)}%`;\n}\n\nconst selectedModel = \"Llama-3.2-3B-Instruct-q4f16_1-MLC\";\nprogressText.textContent = \"Initializing : 0%\"\nconst engine = await webllm.CreateMLCEngine(\n  selectedModel,\n  { initProgressCallback: initProgressCallback }, // engineConfig\n);\nwindow.engine=engine;",
      "status": "[*]",
      "output": "",
      "type": "code"
    },
    {
      "code": "<input type=\"text\" id=\"query\" value=\"Give code for Palindrome in JavaScript\"></input>",
      "status": "",
      "output": "<p><input type=\"text\" id=\"query\" value=\"Give code for Palindrome in JavaScript\"></p>\n",
      "type": "html"
    },
    {
      "code": "let start=new Date()\nlet query=document.getElementById('query').value;\nconst messages = [\n  { role: \"system\", content: \"You are a helpful AI assistant.\" },\n  { role: \"user\", content: query},\n]\n\n// Chunks is an AsyncGenerator object\nconst chunks = await engine.chat.completions.create({\n  messages,\n  temperature: 1,\n  stream: true, // <-- Enable streaming\n  stream_options: { include_usage: true },\n});\n\nlet reply = \"\";\nlet num_tok=0;\nfor await (const chunk of chunks) {\n  reply += chunk.choices[0]?.delta.content || \"\";\n  \tnum_tok=num_tok+1;\n\tscrib.currCell().innerHTML= marked.marked(reply);\n\n}\nlet end=new Date();\nlet time_taken = (end- start)/1000;\nscrib.show(\"num_tok\",num_tok)\nscrib.show(\"Time taken\",time_taken)\nscrib.show(\"tokens per second:\",num_tok/time_taken)\n//scrib.show(reply.usage);",
      "status": "[12]<br><span style=\"font-size:8px\">1.428s<span></span></span>",
      "output": "<p>Here's one:</p>\n<p>What do you call a group of cows playing instruments?</p>\n<p>(wait for it...)</p>\n<p>A MOO-sical band!</p>\n<p>How was that? Did I make you smile?</p>\nnum_tok 39 <br>Time taken 1.427 <br>tokens per second: 27.330063069376312 <br>",
      "type": "code"
    },
    {
      "code": "",
      "status": "",
      "output": "",
      "type": "code"
    }
  ],
  "source": "https://github.com/gopi-suvanam/scribbler",
  "run_on_load": true
}